{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Deep Learning - An√°lisis de Sentimientos\n",
    "\n",
    "Este notebook implementa un modelo de Deep Learning para an√°lisis de sentimientos.\n",
    "\n",
    "## Objetivos:\n",
    "1. **Preparaci√≥n de datos**: Tokenizaci√≥n num√©rica, padding, embeddings\n",
    "2. **Arquitectura**: Red neuronal con capas Embedding, LSTM, Dense\n",
    "3. **Entrenamiento**: Optimizaci√≥n y validaci√≥n\n",
    "4. **Evaluaci√≥n**: M√©tricas y comparaci√≥n con modelos tradicionales\n",
    "5. **Mejoras**: Hiperpar√°metros, regularizaci√≥n, arquitecturas avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# TensorFlow/Keras\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense, Dropout, Bidirectional\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configurar TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow versi√≥n: {tf.__version__}\")\n",
    "print(\"‚úÖ Librer√≠as de Deep Learning importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar y Preparar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para crear dataset sint√©tico\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"Crea un dataset sint√©tico para pruebas\"\"\"\n",
    "    positive_texts = [\n",
    "        \"I love this movie, it is absolutely fantastic!\",\n",
    "        \"Amazing performance by all the actors!\",\n",
    "        \"Great film with excellent cinematography\",\n",
    "        \"Wonderful story and brilliant directing\",\n",
    "        \"Outstanding movie, highly recommended!\",\n",
    "        \"Perfect entertainment for the whole family\",\n",
    "        \"Incredible acting and beautiful soundtrack\",\n",
    "        \"This film exceeded all my expectations\",\n",
    "        \"Masterpiece of modern cinema\",\n",
    "        \"Absolutely loved every minute of it\"\n",
    "    ]\n",
    "    \n",
    "    negative_texts = [\n",
    "        \"This movie is terrible, I hate it completely\",\n",
    "        \"Worst film I have ever seen in my life\",\n",
    "        \"Boring and predictable storyline\",\n",
    "        \"Poor acting and bad direction\",\n",
    "        \"Complete waste of time and money\",\n",
    "        \"Disappointing and poorly executed\",\n",
    "        \"Terrible script and awful performances\",\n",
    "        \"Not worth watching at all\",\n",
    "        \"Very bad movie with no redeeming qualities\",\n",
    "        \"Horrible experience, would not recommend\"\n",
    "    ]\n",
    "    \n",
    "    neutral_texts = [\n",
    "        \"The movie was okay, nothing special\",\n",
    "        \"Average film with some good moments\",\n",
    "        \"Not bad but could have been better\",\n",
    "        \"Decent movie for a casual watch\",\n",
    "        \"It was fine, met my expectations\",\n",
    "        \"Moderate entertainment value\",\n",
    "        \"Acceptable but forgettable\",\n",
    "        \"Standard movie with typical plot\",\n",
    "        \"Neither good nor bad, just average\",\n",
    "        \"Okay for a one-time watch\"\n",
    "    ]\n",
    "    \n",
    "    # Crear dataset\n",
    "    texts = (positive_texts * 50) + (negative_texts * 50) + (neutral_texts * 50)\n",
    "    sentiments = ([4] * 500) + ([0] * 500) + ([2] * 500)\n",
    "    labels = (['Positivo'] * 500) + (['Negativo'] * 500) + (['Neutral'] * 500)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'sentiment': sentiments,\n",
    "        'sentiment_label': labels\n",
    "    })\n",
    "\n",
    "# Cargar dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/dataset_procesado.csv')\n",
    "    print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå No se encontr√≥ el dataset procesado\")\n",
    "    print(\"üí° Creando dataset sint√©tico para pruebas...\")\n",
    "    df = create_synthetic_dataset()\n",
    "    print(f\"‚úÖ Dataset sint√©tico creado: {df.shape[0]} filas\")\n",
    "    \n",
    "# Mostrar informaci√≥n b√°sica\n",
    "print(\"\\n=== INFORMACI√ìN DEL DATASET ===\")\n",
    "print(f\"Total de textos: {len(df)}\")\n",
    "print(\"\\n=== DISTRIBUCI√ìN DE SENTIMIENTOS ===\")\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparando datos para Deep Learning...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Preparar datos para Deep Learning\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Preparando datos para Deep Learning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m texts = \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m      5\u001b[39m labels = df[\u001b[33m'\u001b[39m\u001b[33msentiment_label\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Codificar labels\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Preparar datos para Deep Learning\n",
    "print(\"üîÑ Preparando datos para Deep Learning...\")\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "labels = df['sentiment_label'].tolist()\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Clases: {label_encoder.classes_}\")\n",
    "print(f\"N√∫mero de clases: {num_classes}\")\n",
    "\n",
    "# Convertir a categorical\n",
    "categorical_labels = to_categorical(encoded_labels, num_classes=num_classes)\n",
    "print(f\"Shape de labels categ√≥ricas: {categorical_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizaci√≥n y Preparaci√≥n de Secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar par√°metros\n",
    "MAX_FEATURES = 5000  # Vocabulario m√°ximo\n",
    "MAX_LENGTH = 50      # Longitud m√°xima de secuencia\n",
    "\n",
    "# Crear tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convertir textos a secuencias\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Hacer padding\n",
    "X = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "vocab_size = min(MAX_FEATURES, len(tokenizer.word_index) + 1)\n",
    "print(f\"Vocabulario total: {len(tokenizer.word_index)}\")\n",
    "print(f\"Vocabulario usado: {vocab_size}\")\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Longitud m√°xima: {MAX_LENGTH}\")\n",
    "\n",
    "# Ejemplo de tokenizaci√≥n\n",
    "print(\"\\n=== EJEMPLO DE TOKENIZACI√ìN ===\")\n",
    "print(f\"Texto: {texts[0]}\")\n",
    "print(f\"Secuencia: {sequences[0]}\")\n",
    "print(f\"Padded: {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Divisi√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, categorical_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=np.argmax(y_train, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} muestras\")\n",
    "print(f\"Validation: {X_val.shape[0]} muestras\")\n",
    "print(f\"Test: {X_test.shape[0]} muestras\")\n",
    "\n",
    "print(\"\\n=== DISTRIBUCI√ìN DE CLASES ===\")\n",
    "print(\"Train:\", np.bincount(np.argmax(y_train, axis=1)))\n",
    "print(\"Validation:\", np.bincount(np.argmax(y_val, axis=1)))\n",
    "print(\"Test:\", np.bincount(np.argmax(y_test, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crear Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 128, input_length=MAX_LENGTH),\n",
    "    LSTM(64, return_sequences=True, dropout=0.2),\n",
    "    LSTM(32, dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print(\"=== ARQUITECTURA DEL MODELO ===\")\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal de par√°metros: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "# Entrenar\n",
    "print(\"üîÑ Iniciando entrenamiento...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizaci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de entrenamiento\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training', color='blue')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', color='red')\n",
    "ax1.set_title('Accuracy durante el entrenamiento')\n",
    "ax1.set_xlabel('√âpoca')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Training', color='blue')\n",
    "ax2.plot(history.history['val_loss'], label='Validation', color='red')\n",
    "ax2.set_title('Loss durante el entrenamiento')\n",
    "ax2.set_xlabel('√âpoca')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# M√©tricas finales\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\nTraining Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en test\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"‚úÖ Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"‚úÖ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Reporte\n",
    "print(\"\\n=== REPORTE DE CLASIFICACI√ìN ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Matriz de Confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Matriz de Confusi√≥n - Deep Learning\\nAccuracy: {test_accuracy:.4f}')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Funci√≥n de Predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"Predice el sentimiento de un texto\"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    sentiment = label_encoder.classes_[predicted_class]\n",
    "    return sentiment, confidence\n",
    "\n",
    "# Probar con ejemplos\n",
    "test_texts = [\n",
    "    \"I love this movie! It's fantastic!\",\n",
    "    \"This film is terrible and boring\",\n",
    "    \"The movie was okay, nothing special\",\n",
    "    \"Amazing performance by the actors!\",\n",
    "    \"Not worth watching at all\"\n",
    "]\n",
    "\n",
    "print(\"=== PREDICCIONES DE EJEMPLO ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    sentiment, confidence = predict_sentiment(text)\n",
    "    print(f\"\\n{i}. {text}\")\n",
    "    print(f\"   Predicci√≥n: {sentiment} (Confianza: {confidence:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('../data/models', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "model.save('../data/models/deep_learning_model.h5')\n",
    "\n",
    "# Guardar tokenizer\n",
    "with open('../data/models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Guardar label encoder\n",
    "with open('../data/models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"\\n‚úÖ Modelo guardado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ RESUMEN FINAL - DEEP LEARNING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä Dataset: {len(texts)} textos procesados\")\n",
    "print(f\"üèóÔ∏è  Arquitectura: LSTM + Dense layers\")\n",
    "print(f\"üìà Par√°metros: {total_params:,}\")\n",
    "print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"‚ö° √âpocas: {len(history.history['accuracy'])}\")\n",
    "print(f\"üî§ Vocabulario: {vocab_size} palabras\")\n",
    "print(f\"üìè Longitud m√°xima: {MAX_LENGTH} tokens\")\n",
    "print(f\"üè∑Ô∏è  Clases: {', '.join(label_encoder.classes_)}\")\n",
    "print(\"\\n‚úÖ Modelo completado y guardado!\")\n",
    "print(\"\\nüí° Pr√≥ximos pasos:\")\n",
    "print(\"   - Probar embeddings pre-entrenados\")\n",
    "print(\"   - Experimentar con CNN o Transformer\")\n",
    "print(\"   - Optimizar hiperpar√°metros\")\n",
    "print(\"   - Crear interfaz web\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
