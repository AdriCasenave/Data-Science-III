{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî§ An√°lisis de NLP Tradicional\n",
    "\n",
    "Este notebook implementa las t√©cnicas tradicionales de procesamiento de lenguaje natural.\n",
    "\n",
    "## Objetivos:\n",
    "1. **Preprocessing**: Tokenizaci√≥n, limpieza, normalizaci√≥n\n",
    "2. **An√°lisis de frecuencias**: Palabras m√°s comunes, n-gramas\n",
    "3. **An√°lisis de sentimientos**: Usando VADER y TextBlob\n",
    "4. **Visualizaciones**: Word clouds, gr√°ficos de barras\n",
    "5. **Modelo baseline**: Clasificador con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as de NLP importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Importar todas las librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk_downloads = ['punkt', 'stopwords', 'wordnet', 'vader_lexicon', 'omw-1.4']\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.download(item, quiet=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de NLP importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado: 1000 filas, 5 columnas\n",
      "\n",
      "=== INFORMACI√ìN DEL DATASET ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             1000 non-null   object\n",
      " 1   sentiment        1000 non-null   int64 \n",
      " 2   sentiment_label  1000 non-null   object\n",
      " 3   text_length      1000 non-null   int64 \n",
      " 4   word_count       1000 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 39.2+ KB\n",
      "None\n",
      "\n",
      "=== DISTRIBUCI√ìN DE SENTIMIENTOS ===\n",
      "sentiment_label\n",
      "Negativo    400\n",
      "Positivo    400\n",
      "Neutral     200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset procesado\n",
    "try:\n",
    "    df = pd.read_csv('../data/dataset_procesado.csv')\n",
    "    print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå No se encontr√≥ el dataset procesado\")\n",
    "    print(\"üí° Ejecuta primero el notebook 01_exploracion_datos.ipynb\")\n",
    "    \n",
    "# Mostrar informaci√≥n b√°sica\n",
    "print(\"\\n=== INFORMACI√ìN DEL DATASET ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== DISTRIBUCI√ìN DE SENTIMIENTOS ===\")\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Aplicando preprocessing...\n",
      "\n",
      "=== EJEMPLOS DE PREPROCESSING ===\n",
      "\n",
      "1. Original: I hate this movie, it was terrible...\n",
      "   Procesado: hate movie terrible...\n",
      "\n",
      "2. Original: Amazing film! Loved every minute of it...\n",
      "   Procesado: amazing film loved every minute...\n",
      "\n",
      "3. Original: Worst experience ever, very disappointed...\n",
      "   Procesado: worst experience ever disappointed...\n",
      "\n",
      "‚úÖ Preprocessing completado\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n de preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Aplica preprocessing b√°sico al texto:\n",
    "    - Convierte a min√∫sculas\n",
    "    - Elimina URLs, menciones, hashtags\n",
    "    - Elimina caracteres especiales\n",
    "    - Tokeniza y elimina stopwords\n",
    "    - Aplica lemmatizaci√≥n\n",
    "    \"\"\"\n",
    "    # Inicializar lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Eliminar menciones y hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Eliminar caracteres especiales y n√∫meros\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Eliminar stopwords y palabras muy cortas\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatizaci√≥n\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar preprocessing\n",
    "print(\"üîÑ Aplicando preprocessing...\")\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n=== EJEMPLOS DE PREPROCESSING ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Original: {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"   Procesado: {df['text_clean'].iloc[i][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lisis de Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 15 PALABRAS - POSITIVO ===\n",
      "amazing         :  100\n",
      "film            :  100\n",
      "loved           :  100\n",
      "every           :  100\n",
      "minute          :  100\n",
      "fantastic       :  100\n",
      "best            :  100\n",
      "movie           :  100\n",
      "seen            :  100\n",
      "year            :  100\n",
      "excellent       :  100\n",
      "quality         :  100\n",
      "highly          :  100\n",
      "recommend       :  100\n",
      "outstanding     :  100\n",
      "\n",
      "=== TOP 15 PALABRAS - NEGATIVO ===\n",
      "hate            :  100\n",
      "movie           :  100\n",
      "terrible        :  100\n",
      "worst           :  100\n",
      "experience      :  100\n",
      "ever            :  100\n",
      "disappointed    :  100\n",
      "bad             :  100\n",
      "would           :  100\n",
      "recommend       :  100\n",
      "boring          :  100\n",
      "predictable     :  100\n",
      "plot            :  100\n",
      "\n",
      "=== TOP 15 PALABRAS - NEUTRAL ===\n",
      "okay            :  100\n",
      "nothing         :  100\n",
      "special         :  100\n",
      "average         :  100\n",
      "movie           :  100\n",
      "bad             :  100\n",
      "good            :  100\n",
      "\n",
      "=== TOP 20 PALABRAS GENERALES ===\n",
      "movie           :  300\n",
      "bad             :  200\n",
      "recommend       :  200\n",
      "hate            :  100\n",
      "terrible        :  100\n",
      "amazing         :  100\n",
      "film            :  100\n",
      "loved           :  100\n",
      "every           :  100\n",
      "minute          :  100\n",
      "worst           :  100\n",
      "experience      :  100\n",
      "ever            :  100\n",
      "disappointed    :  100\n",
      "fantastic       :  100\n",
      "best            :  100\n",
      "seen            :  100\n",
      "year            :  100\n",
      "would           :  100\n",
      "excellent       :  100\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de palabras m√°s frecuentes\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"\n",
    "    Obtiene las palabras m√°s frecuentes\n",
    "    \"\"\"\n",
    "    all_words = ' '.join(texts).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# Palabras m√°s frecuentes por sentimiento\n",
    "sentiments = ['Positivo', 'Negativo', 'Neutral']\n",
    "top_words_by_sentiment = {}\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    if sentiment in df['sentiment_label'].values:\n",
    "        texts = df[df['sentiment_label'] == sentiment]['text_clean'].tolist()\n",
    "        top_words = get_top_words(texts, 15)\n",
    "        top_words_by_sentiment[sentiment] = top_words\n",
    "        \n",
    "        print(f\"\\n=== TOP 15 PALABRAS - {sentiment.upper()} ===\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word:15} : {count:4d}\")\n",
    "\n",
    "# Palabras m√°s frecuentes en general\n",
    "print(\"\\n=== TOP 20 PALABRAS GENERALES ===\")\n",
    "general_top_words = get_top_words(df['text_clean'].tolist(), 20)\n",
    "for word, count in general_top_words:\n",
    "    print(f\"{word:15} : {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelo Baseline con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparando datos para el modelo...\n",
      "Datos de entrenamiento: 800\n",
      "Datos de prueba: 200\n",
      "Distribuci√≥n de entrenamiento:\n",
      "sentiment_label\n",
      "Negativo    320\n",
      "Positivo    320\n",
      "Neutral     160\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preparar datos para el modelo\n",
    "print(\"üîÑ Preparando datos para el modelo...\")\n",
    "\n",
    "# Usar texto limpio\n",
    "X = df['text_clean']\n",
    "y = df['sentiment_label']\n",
    "\n",
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(X_train)}\")\n",
    "print(f\"Datos de prueba: {len(X_test)}\")\n",
    "print(f\"Distribuci√≥n de entrenamiento:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Entrenando modelo Naive Bayes...\n",
      "\n",
      "‚úÖ Accuracy Naive Bayes: 1.000\n",
      "\n",
      "=== REPORTE DE CLASIFICACI√ìN - NAIVE BAYES ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       1.00      1.00      1.00        80\n",
      "     Neutral       1.00      1.00      1.00        40\n",
      "    Positivo       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear pipeline con TF-IDF + Naive Bayes\n",
    "pipeline_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Entrenar modelo\n",
    "print(\"üîÑ Entrenando modelo Naive Bayes...\")\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_nb = pipeline_nb.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"\\n‚úÖ Accuracy Naive Bayes: {accuracy_nb:.3f}\")\n",
    "\n",
    "print(\"\\n=== REPORTE DE CLASIFICACI√ìN - NAIVE BAYES ===\")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guardado de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Modelo guardado en '../data/models/'\n",
      "\n",
      "=== RESUMEN FINAL ===\n",
      "Modelo: Naive Bayes con TF-IDF\n",
      "Accuracy: 1.000\n",
      "Dataset procesado: 1000 textos\n",
      "Caracter√≠sticas extra√≠das: TF-IDF con 5000 features\n",
      "\n",
      "üéâ An√°lisis de NLP tradicional completado!\n"
     ]
    }
   ],
   "source": [
    "# Guardar modelos y resultados\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Crear directorio para modelos\n",
    "os.makedirs('../data/models', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(pipeline_nb, '../data/models/naive_bayes_model.pkl')\n",
    "\n",
    "print(\"\\n‚úÖ Modelo guardado en '../data/models/'\")\n",
    "print(\"\\n=== RESUMEN FINAL ===\")\n",
    "print(f\"Modelo: Naive Bayes con TF-IDF\")\n",
    "print(f\"Accuracy: {accuracy_nb:.3f}\")\n",
    "print(f\"Dataset procesado: {len(df)} textos\")\n",
    "print(f\"Caracter√≠sticas extra√≠das: TF-IDF con {pipeline_nb.named_steps['tfidf'].max_features} features\")\n",
    "print(\"\\nüéâ An√°lisis de NLP tradicional completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
